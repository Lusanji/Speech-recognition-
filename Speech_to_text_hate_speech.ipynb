{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRhTDO1k_XYG"
      },
      "source": [
        "#Installing dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl8AiMTloTTo",
        "outputId": "54f463d9-ed4d-4106-a881-15564320e8cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed May 17 13:32:44 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fSOEhMM_Wz9",
        "outputId": "f3b6a2f5-2a8d-4390-e95d-2e9fbff21d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.11\n",
            "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (3.8.0)\n",
            "Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11)\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (23.1)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (1.16.0)\n",
            "Collecting tensorboard<2.12,>=2.11 (from tensorflow==2.11)\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow==2.11)\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow==2.11)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.2\n",
            "    Uninstalling tensorboard-2.12.2:\n",
            "      Successfully uninstalled tensorboard-2.12.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tensorflow==2.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LegQHNOsNrHR",
        "outputId": "cf0b42e9-7c60-4654-a328-0cb842afc85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f5N2DjabN8sO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nRXJ54Yg_cCZ",
        "outputId": "b59672f7-2d4d-4ce4-8b1a-b575c9fddde6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: conda: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycocotools==2.0.0\n",
            "  Downloading pycocotools-2.0.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.0-cp310-cp310-linux_x86_64.whl size=386202 sha256=87cc25e8e684fb9c3882a7c81c003ee7871f832357410484a6d120d815f28482\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/75/00/695664abbb99dee72b09b0f9a682345c1545dad9f1f2998ab6\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.6\n",
            "    Uninstalling pycocotools-2.0.6:\n",
            "      Successfully uninstalled pycocotools-2.0.6\n",
            "Successfully installed pycocotools-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.0.post2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Collecting numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 (from librosa)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scipy==1.2.0\n",
            "  Downloading scipy-1.2.0.tar.gz (23.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: scipy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for scipy (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scipy\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for scipy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py clean\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for scipy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build scipy\n",
            "\u001b[31mERROR: Could not build wheels for scipy, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install --upgrade numpy\n",
        "!conda install -y -c conda-forge numpy\n",
        "!pip install pycocotools==2.0.0\n",
        "%pip install librosa\n",
        "!pip install -U scipy==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lE6Dkg28_f-H"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "from numpy.random import rand\n",
        "import numpy as np                        # for handling large arrays\n",
        "import random\n",
        "\n",
        "rng = np.random.RandomState (seed = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WcTg5zMY_j_Y",
        "outputId": "2087f1a9-62d9-4dc7-826b-5e4f95dc68a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting praat-parselmouth\n",
            "  Downloading praat_parselmouth-0.4.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from praat-parselmouth) (1.23.5)\n",
            "Installing collected packages: praat-parselmouth\n",
            "Successfully installed praat-parselmouth-0.4.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.0.post2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['random']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
          ]
        }
      ],
      "source": [
        "from IPython.core.display import display\n",
        "from tqdm import tqdm_notebook as tqdm \n",
        "from tqdm import tnrange as trange\n",
        "from IPython.display import Audio         # for playing audio\n",
        "from numpy.random import randint\n",
        "import matplotlib.pyplot as plt           # plotting with Matlab functionality\n",
        "!pip install praat-parselmouth\n",
        "import os, time, csv, datetime            # for accessing local files, time and date\n",
        "import IPython.display as ipd\n",
        "from scipy.io import wavfile\n",
        "from numpy import matrix\n",
        "%pip install tensorflow\n",
        "from sklearn import svm\n",
        "import librosa.display                    # librosa plot functions\n",
        "from numpy import diff \n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "%pip install librosa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%pip install keras\n",
        "%matplotlib inline\n",
        "import parselmouth\n",
        "import statistics\n",
        "import seaborn\n",
        "import pathlib\n",
        "import librosa                            # for data manipulation and analysis\n",
        "import pickle\n",
        "import shutil\n",
        "import scipy                              # for common math functions\n",
        "import h5py                               # data visualization based on matplotlib\n",
        "import math\n",
        "import re\n",
        "\n",
        "# Preprocessing\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Dense\n",
        "from tensorflow import keras\n",
        "from sklearn import metrics \n",
        "from sklearn import tree\n",
        "import sklearn as sk\n",
        "import sklearn                                # a machine learning library\n",
        "%pylab inline\n",
        "\n",
        "# stat libraries\n",
        "from scipy.stats import kurtosis\n",
        "from scipy.stats import skew\n",
        "from scipy import stats\n",
        "from scipy import misc\n",
        "\n",
        "# Libraries for the evaluation\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4RAIcnbFgTfa",
        "outputId": "2e784377-11f2-4c23-da40-5fa536adb0c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -qqq install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yn3b17BZYYZK"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import os\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nJ0Yt7rOCl3i",
        "outputId": "20059109-deb9-498d-ce5c-0f99a8ce2a89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:25<00:00, 119MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# Import the libraries \n",
        "import whisper\n",
        "\n",
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model \n",
        "whisper_model = whisper.load_model(\"large\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4HjOru0ZtYY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMdDHXwGZuvm"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/Kiswahili-Train.zip -d /content/Kiswahili-Train\n",
        "!unzip /content/drive/MyDrive/English-Train.zip -d /content/English-Train\n",
        "!unzip /content/drive/MyDrive/Kiswahili-Test.zip -d /content/Kiswahili-Test\n",
        "!unzip /content/drive/MyDrive/English-Test.zip -d /content/English-Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoiV_bV4_w7y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXXFownt_20O"
      },
      "outputs": [],
      "source": [
        "#Get inside directory \n",
        "import os\n",
        "\n",
        "parent_dir = os.getcwd()\n",
        "dataset_dir = os.path.join(parent_dir,'/content/drive/MyDrive/English-Train.zip')\n",
        "print(parent_dir)\n",
        "print(dataset_dir)\n",
        "\n",
        "!dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRAJhSh8_3Fc"
      },
      "source": [
        "#Reading audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q5eUUKD-EwL"
      },
      "outputs": [],
      "source": [
        "file_name='/content/English-Train/Hate/1.wav'\n",
        "\n",
        "audio_data, sampling_rate = librosa.load(file_name)\n",
        "librosa.display.waveshow(audio_data,sr=sampling_rate)\n",
        "ipd.Audio(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J-BRF0C-Ezk"
      },
      "outputs": [],
      "source": [
        "file_name='/content/English-Train/Normal/1.wav'\n",
        "\n",
        "audio_data, sampling_rate = librosa.load(file_name)\n",
        "librosa.display.waveshow(audio_data,sr=sampling_rate)\n",
        "ipd.Audio(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJR5yDnN__Y-"
      },
      "source": [
        "#Speech to text conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmW8TooqNBZ8"
      },
      "outputs": [],
      "source": [
        "audio_file = \"/content/English-Train/Hate/1.wav\"\n",
        "result = whisper_model.transcribe(audio_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJjFLj1NcEh7"
      },
      "outputs": [],
      "source": [
        "tx = result[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnrQYJI3cPmH",
        "outputId": "619e624e-18d1-4845-f8de-fdc2a444c545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " picture for your side panel. You know people only follow you to incessantly mock you, right? I'm sure that's what it is because I don't have a lot of followers. I got about 400 and a little over like 350, 400 range. And that's, they're not following me to make fun of me because I just started doing these posts like the other day. Yeah, okay. What does your best friend say when you talk shit about his race or do you keep your mouth shut around him? My best friend, his name is Marcel. Marcel Bichelle. No. He is practically white. Like he is, his skin color is almost, if it was dark in the room you wouldn't be able to find him. He's that black. His parents, they're both surgeons. He has a wealthy family. He hangs out with all white people. He makes fun of his race all the time because he knows that there are niggers who are just ignorant fucks who are doing nothing but make fun of him.\n"
          ]
        }
      ],
      "source": [
        "print(tx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnZU3GzdcXiL"
      },
      "outputs": [],
      "source": [
        "tx = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31C6XDimc6Ko"
      },
      "outputs": [],
      "source": [
        "tx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJUz35X6bGEv"
      },
      "outputs": [],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w86OK0vgX2_9"
      },
      "outputs": [],
      "source": [
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q_PWDYycumx"
      },
      "outputs": [],
      "source": [
        "result[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr8PcOq0zIWm"
      },
      "outputs": [],
      "source": [
        "#loop folder\n",
        "import os\n",
        "\n",
        "folder_path = \"/content/English-Train\"\n",
        "\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "  for dir in dirs:\n",
        "    if dir == \"Hate\" or dir == \"Normal\":\n",
        "      dir_path = os.path.join(root, dir)\n",
        "      print(f\"Contents of {dir_path}:\")\n",
        "      for file in os.listdir(dir_path):\n",
        "        print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NduCES1QzJSQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# set the path to the parent folder containing the two subfolders\n",
        "parent_folder = '/content/English-Train'\n",
        "\n",
        "# get a list of all files in subfolder1 and subfolder2\n",
        "subfolder1_files = glob.glob(os.path.join(parent_folder, 'Hate', '*'))\n",
        "subfolder2_files = glob.glob(os.path.join(parent_folder, 'Normal', '*'))\n",
        "\n",
        "# combine the two lists of files\n",
        "all_files = subfolder1_files + subfolder2_files\n",
        "\n",
        "# loop through each file and do something with it\n",
        "for file in all_files:\n",
        "    # do something with the file, e.g. print its name\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-pvomdzAH29"
      },
      "source": [
        "#English language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2wD6vDZ5v0w"
      },
      "source": [
        "##English transcribed text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDWVssdQCBrj"
      },
      "outputs": [],
      "source": [
        "def speech_to_text(audio_path):\n",
        "   \n",
        "    try:\n",
        "        result = whisper_model.transcribe(audio_path)\n",
        "        result[\"text\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file)\n",
        "        return None\n",
        "     \n",
        "    return result[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyvBAbsuUswv",
        "outputId": "7e5dfc03-87bc-447a-e101-e48459371539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished feature extraction from  72  files\n"
          ]
        }
      ],
      "source": [
        "from ssl import OPENSSL_VERSION_INFO\n",
        "import csv\n",
        "import scipy.signal\n",
        "import scipy.fftpack\n",
        "\n",
        "\n",
        "# Load various imports \n",
        "\n",
        "dt = []\n",
        "\n",
        "labels = 'Hate Normal'.split()\n",
        "\n",
        "# Iterate through each sound file and extract the features \n",
        "for k in labels:\n",
        "    for filename in os.listdir(f'/content/English-Train/{k}'):\n",
        "        audio_path = f'/content/English-Train/{k}/{filename}'\n",
        "      \n",
        "        result = whisper_model.transcribe(audio_path)\n",
        "        dt.append([filename, result[\"text\"], k])\n",
        "\n",
        "# Convert into a Panda dataframe \n",
        "df = pd.DataFrame(dt, columns=['filename', 'text','label'])\n",
        "\n",
        "\n",
        "print('Finished feature extraction from ', len(df), ' files')\n",
        "df = df.to_csv('/content/E-text.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI35lW4SQrdq"
      },
      "source": [
        "##Preprocess text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1ppqPyfPXCc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('E-text.csv', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJhZgV9kAfpj",
        "outputId": "aa7b5750-4731-4de7-99e2-9bce4c9cbd9a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-20f0be81-6a36-4884-9442-5f41fd8b0ed8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24.wav</td>\n",
              "      <td>So I'm going to read a few comments that demo...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.wav</td>\n",
              "      <td>Okay, so, this is my second time recording th...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20.wav</td>\n",
              "      <td>it's a jungle out there in our city after mid...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34.wav</td>\n",
              "      <td>And he constantly pushes this out to a large ...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27.wav</td>\n",
              "      <td>My wife had to spend time at the UP hospital ...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20f0be81-6a36-4884-9442-5f41fd8b0ed8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20f0be81-6a36-4884-9442-5f41fd8b0ed8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20f0be81-6a36-4884-9442-5f41fd8b0ed8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  filename                                               text label\n",
              "0   24.wav   So I'm going to read a few comments that demo...  Hate\n",
              "1    3.wav   Okay, so, this is my second time recording th...  Hate\n",
              "2   20.wav   it's a jungle out there in our city after mid...  Hate\n",
              "3   34.wav   And he constantly pushes this out to a large ...  Hate\n",
              "4   27.wav   My wife had to spend time at the UP hospital ...  Hate"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiEFl0CXAoxF",
        "outputId": "3f36ba11-6eeb-43dd-c8b3-5b2755666a5d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-87e1fe14-b9e5-430a-aec8-96e203076428\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So I'm going to read a few comments that demo...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Okay, so, this is my second time recording th...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it's a jungle out there in our city after mid...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>And he constantly pushes this out to a large ...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My wife had to spend time at the UP hospital ...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87e1fe14-b9e5-430a-aec8-96e203076428')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87e1fe14-b9e5-430a-aec8-96e203076428 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87e1fe14-b9e5-430a-aec8-96e203076428');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text label\n",
              "0   So I'm going to read a few comments that demo...  Hate\n",
              "1   Okay, so, this is my second time recording th...  Hate\n",
              "2   it's a jungle out there in our city after mid...  Hate\n",
              "3   And he constantly pushes this out to a large ...  Hate\n",
              "4   My wife had to spend time at the UP hospital ...  Hate"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.drop(['filename'], axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTeWkfU9exzr",
        "outputId": "322a2857-f0d6-4a75-8a54-aae73ce0b9f5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0e2175f8-8901-48de-927c-3fa157903704\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So I'm going to read a few comments that demo...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Okay, so, this is my second time recording th...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it's a jungle out there in our city after mid...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>And he constantly pushes this out to a large ...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My wife had to spend time at the UP hospital ...</td>\n",
              "      <td>Hate</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e2175f8-8901-48de-927c-3fa157903704')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e2175f8-8901-48de-927c-3fa157903704 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e2175f8-8901-48de-927c-3fa157903704');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text label\n",
              "0   So I'm going to read a few comments that demo...  Hate\n",
              "1   Okay, so, this is my second time recording th...  Hate\n",
              "2   it's a jungle out there in our city after mid...  Hate\n",
              "3   And he constantly pushes this out to a large ...  Hate\n",
              "4   My wife had to spend time at the UP hospital ...  Hate"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwlDaGqWZeR_",
        "outputId": "30b230af-ab34-4c2f-af3e-b62ca7b214cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Normal    0.513889\n",
              "Hate      0.486111\n",
              "Name: label, dtype: float64"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJJpocHAffMH"
      },
      "outputs": [],
      "source": [
        "df.replace(('Hate', 'Normal'), (0, 1), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvzBw98_fO-M",
        "outputId": "6daf7458-21f0-44ca-8e7c-6d2b9026b9a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0300ae1a-890d-4f9a-b9d7-f4a3d5a8258d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So I'm going to read a few comments that demo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Okay, so, this is my second time recording th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it's a jungle out there in our city after mid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>And he constantly pushes this out to a large ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My wife had to spend time at the UP hospital ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0300ae1a-890d-4f9a-b9d7-f4a3d5a8258d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0300ae1a-890d-4f9a-b9d7-f4a3d5a8258d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0300ae1a-890d-4f9a-b9d7-f4a3d5a8258d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text  label\n",
              "0   So I'm going to read a few comments that demo...      0\n",
              "1   Okay, so, this is my second time recording th...      0\n",
              "2   it's a jungle out there in our city after mid...      0\n",
              "3   And he constantly pushes this out to a large ...      0\n",
              "4   My wife had to spend time at the UP hospital ...      0"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70W8hre6872l",
        "outputId": "0b50f166-145a-411e-c07c-3e4d7b74097e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for python\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.10/dist-packages (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "import re \n",
        "import os\n",
        "import string \n",
        "!pip install nltk \n",
        "!pip install python \n",
        "!pip install autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjdS2DC46Zd1"
      },
      "outputs": [],
      "source": [
        "def wordopt(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub(\"\\\\W\",\" \",text) \n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw48RRl_7Abt"
      },
      "outputs": [],
      "source": [
        "df[\"text\"] = df[\"text\"].apply(wordopt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__EHzyR_9IDy",
        "outputId": "3cb51a7e-d6f5-4a70-8fae-d148f7bee6e5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fad7cf4f-abee-4891-9250-1adfc3e76e36\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>so i m going to read a few comments that demo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>okay  so  this is my second time recording th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it s a jungle out there in our city after mid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>and he constantly pushes this out to a large ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>my wife had to spend time at the up hospital ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fad7cf4f-abee-4891-9250-1adfc3e76e36')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fad7cf4f-abee-4891-9250-1adfc3e76e36 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fad7cf4f-abee-4891-9250-1adfc3e76e36');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text  label\n",
              "0   so i m going to read a few comments that demo...      0\n",
              "1   okay  so  this is my second time recording th...      0\n",
              "2   it s a jungle out there in our city after mid...      0\n",
              "3   and he constantly pushes this out to a large ...      0\n",
              "4   my wife had to spend time at the up hospital ...      0"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUnR02GFOOgq",
        "outputId": "dc21347d-1c15-4b1a-a79b-3f766cd6b7b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    0.513889\n",
              "0    0.486111\n",
              "Name: label, dtype: float64"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btPhNRchOfb0"
      },
      "source": [
        "##Train models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdf3ncAElahT"
      },
      "source": [
        "##Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ryEnOjVOVPo"
      },
      "source": [
        "Split dataset into train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pi62z7dAr4j"
      },
      "outputs": [],
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZzWLHumOhap",
        "outputId": "133fd816-6bb4-4504-bac9-8be4baa8cb39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQdi-uKvDrNb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# sample data\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TluwdhUSPbM1",
        "outputId": "15cea27c-63c7-486c-e54d-6c2a735b2b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# output\n",
        "print(sent_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRzDp7pCPkey"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRVnWFXIPgFZ",
        "outputId": "ad5896ce-5dbc-433d-b556-6bef7b07de36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af15mDP0PnA-"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1R0tqd7aoUj",
        "outputId": "420da075-f896-44c9-bb07-34dc27143826"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQrLM-dna0YQ"
      },
      "source": [
        "Convert integer sequences to tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeVUzszDOduZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3mJw6rjhdNp"
      },
      "source": [
        "Create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r_QQBwVOdzh"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpxbjyHvhlEk"
      },
      "source": [
        "Freeze bert parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmouN2XlhgU1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhmeUtY6hrqa"
      },
      "source": [
        "Define model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9eniI3hhgdR"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nArln8MThgky"
      },
      "outputs": [],
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgr_nf6ph43P",
        "outputId": "d2efb83f-08d7-4276-9f46-a9ae8750d548"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKCU3_0gh-z4"
      },
      "source": [
        "Find class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE4nG8cmh5rv",
        "outputId": "00d529d3-47b7-43d7-d627-25501ef7c420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.04166667 0.96153846]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels                                                   \n",
        "                                    )\n",
        "print(class_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dND7SnC-Od2t"
      },
      "outputs": [],
      "source": [
        "\n",
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4pj-CP2jcuc"
      },
      "source": [
        "Fine tune bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szHcgQzajX53"
      },
      "outputs": [],
      "source": [
        "\n",
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUphg-h4jj98"
      },
      "outputs": [],
      "source": [
        "\n",
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-lOnWpQjqrE"
      },
      "source": [
        "Start model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECvvicggjYNR",
        "outputId": "02ed726c-0e70-4dad-e2b3-0d2e469a499d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.669\n",
            "Validation Loss: 0.761\n",
            "\n",
            " Epoch 2 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.706\n",
            "Validation Loss: 1.032\n",
            "\n",
            " Epoch 3 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.878\n",
            "Validation Loss: 0.864\n",
            "\n",
            " Epoch 4 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.662\n",
            "Validation Loss: 0.813\n",
            "\n",
            " Epoch 5 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.888\n",
            "Validation Loss: 0.923\n",
            "\n",
            " Epoch 6 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.820\n",
            "Validation Loss: 0.718\n",
            "\n",
            " Epoch 7 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.667\n",
            "Validation Loss: 0.703\n",
            "\n",
            " Epoch 8 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.701\n",
            "Validation Loss: 0.757\n",
            "\n",
            " Epoch 9 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.726\n",
            "Validation Loss: 0.671\n",
            "\n",
            " Epoch 10 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.657\n",
            "Validation Loss: 0.676\n",
            "\n",
            " Epoch 11 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.641\n",
            "Validation Loss: 0.673\n",
            "\n",
            " Epoch 12 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.622\n",
            "Validation Loss: 0.669\n",
            "\n",
            " Epoch 13 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.595\n",
            "Validation Loss: 0.717\n",
            "\n",
            " Epoch 14 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.608\n",
            "Validation Loss: 0.670\n",
            "\n",
            " Epoch 15 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.595\n",
            "Validation Loss: 0.650\n",
            "\n",
            " Epoch 16 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.653\n",
            "Validation Loss: 0.660\n",
            "\n",
            " Epoch 17 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.634\n",
            "Validation Loss: 0.610\n",
            "\n",
            " Epoch 18 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.548\n",
            "Validation Loss: 0.632\n",
            "\n",
            " Epoch 19 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.579\n",
            "Validation Loss: 0.580\n",
            "\n",
            " Epoch 20 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.549\n",
            "Validation Loss: 0.600\n",
            "\n",
            " Epoch 21 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.583\n",
            "Validation Loss: 0.589\n",
            "\n",
            " Epoch 22 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.520\n",
            "Validation Loss: 0.606\n",
            "\n",
            " Epoch 23 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.603\n",
            "Validation Loss: 0.649\n",
            "\n",
            " Epoch 24 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.496\n",
            "Validation Loss: 0.574\n",
            "\n",
            " Epoch 25 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.519\n",
            "Validation Loss: 0.635\n",
            "\n",
            " Epoch 26 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.519\n",
            "Validation Loss: 0.560\n",
            "\n",
            " Epoch 27 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.517\n",
            "Validation Loss: 0.576\n",
            "\n",
            " Epoch 28 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.496\n",
            "Validation Loss: 0.514\n",
            "\n",
            " Epoch 29 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.552\n",
            "Validation Loss: 0.631\n",
            "\n",
            " Epoch 30 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.564\n",
            "Validation Loss: 0.553\n",
            "\n",
            " Epoch 31 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.446\n",
            "Validation Loss: 0.524\n",
            "\n",
            " Epoch 32 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.499\n",
            "Validation Loss: 0.507\n",
            "\n",
            " Epoch 33 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.432\n",
            "Validation Loss: 0.510\n",
            "\n",
            " Epoch 34 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.369\n",
            "Validation Loss: 0.500\n",
            "\n",
            " Epoch 35 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.391\n",
            "Validation Loss: 0.514\n",
            "\n",
            " Epoch 36 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.371\n",
            "Validation Loss: 0.499\n",
            "\n",
            " Epoch 37 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.442\n",
            "Validation Loss: 0.483\n",
            "\n",
            " Epoch 38 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.360\n",
            "Validation Loss: 0.499\n",
            "\n",
            " Epoch 39 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.395\n",
            "Validation Loss: 0.465\n",
            "\n",
            " Epoch 40 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.328\n",
            "Validation Loss: 0.498\n",
            "\n",
            " Epoch 41 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.393\n",
            "Validation Loss: 0.524\n",
            "\n",
            " Epoch 42 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.351\n",
            "Validation Loss: 0.522\n",
            "\n",
            " Epoch 43 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.321\n",
            "Validation Loss: 0.490\n",
            "\n",
            " Epoch 44 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.332\n",
            "Validation Loss: 0.550\n",
            "\n",
            " Epoch 45 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.355\n",
            "Validation Loss: 0.480\n",
            "\n",
            " Epoch 46 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.395\n",
            "Validation Loss: 0.631\n",
            "\n",
            " Epoch 47 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.463\n",
            "Validation Loss: 0.511\n",
            "\n",
            " Epoch 48 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.382\n",
            "Validation Loss: 0.486\n",
            "\n",
            " Epoch 49 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.383\n",
            "Validation Loss: 0.453\n",
            "\n",
            " Epoch 50 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.297\n",
            "Validation Loss: 0.641\n",
            "\n",
            " Epoch 51 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.367\n",
            "Validation Loss: 0.488\n",
            "\n",
            " Epoch 52 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.285\n",
            "Validation Loss: 0.508\n",
            "\n",
            " Epoch 53 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.404\n",
            "Validation Loss: 0.468\n",
            "\n",
            " Epoch 54 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.313\n",
            "Validation Loss: 0.732\n",
            "\n",
            " Epoch 55 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.383\n",
            "Validation Loss: 0.564\n",
            "\n",
            " Epoch 56 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.325\n",
            "Validation Loss: 0.518\n",
            "\n",
            " Epoch 57 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.249\n",
            "Validation Loss: 0.490\n",
            "\n",
            " Epoch 58 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.308\n",
            "Validation Loss: 0.676\n",
            "\n",
            " Epoch 59 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.323\n",
            "Validation Loss: 0.537\n",
            "\n",
            " Epoch 60 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.321\n",
            "Validation Loss: 0.605\n",
            "\n",
            " Epoch 61 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.402\n",
            "Validation Loss: 0.508\n",
            "\n",
            " Epoch 62 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.259\n",
            "Validation Loss: 0.726\n",
            "\n",
            " Epoch 63 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.563\n",
            "Validation Loss: 0.779\n",
            "\n",
            " Epoch 64 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.487\n",
            "Validation Loss: 0.504\n",
            "\n",
            " Epoch 65 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.366\n",
            "Validation Loss: 0.520\n",
            "\n",
            " Epoch 66 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.274\n",
            "Validation Loss: 0.517\n",
            "\n",
            " Epoch 67 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.306\n",
            "Validation Loss: 0.542\n",
            "\n",
            " Epoch 68 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.234\n",
            "Validation Loss: 0.464\n",
            "\n",
            " Epoch 69 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.412\n",
            "Validation Loss: 0.519\n",
            "\n",
            " Epoch 70 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.324\n",
            "Validation Loss: 0.493\n",
            "\n",
            " Epoch 71 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.207\n",
            "Validation Loss: 0.811\n",
            "\n",
            " Epoch 72 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.361\n",
            "Validation Loss: 0.573\n",
            "\n",
            " Epoch 73 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.258\n",
            "Validation Loss: 0.490\n",
            "\n",
            " Epoch 74 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.348\n",
            "Validation Loss: 0.486\n",
            "\n",
            " Epoch 75 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.251\n",
            "Validation Loss: 0.519\n",
            "\n",
            " Epoch 76 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.235\n",
            "Validation Loss: 0.489\n",
            "\n",
            " Epoch 77 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.189\n",
            "Validation Loss: 0.503\n",
            "\n",
            " Epoch 78 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.239\n",
            "Validation Loss: 0.504\n",
            "\n",
            " Epoch 79 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.232\n",
            "Validation Loss: 0.509\n",
            "\n",
            " Epoch 80 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.216\n",
            "Validation Loss: 0.472\n",
            "\n",
            " Epoch 81 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.204\n",
            "Validation Loss: 0.446\n",
            "\n",
            " Epoch 82 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.162\n",
            "Validation Loss: 0.486\n",
            "\n",
            " Epoch 83 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.130\n",
            "Validation Loss: 0.472\n",
            "\n",
            " Epoch 84 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.198\n",
            "Validation Loss: 0.433\n",
            "\n",
            " Epoch 85 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.234\n",
            "Validation Loss: 0.455\n",
            "\n",
            " Epoch 86 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.125\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 87 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.186\n",
            "Validation Loss: 0.588\n",
            "\n",
            " Epoch 88 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.256\n",
            "Validation Loss: 0.581\n",
            "\n",
            " Epoch 89 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.257\n",
            "Validation Loss: 0.528\n",
            "\n",
            " Epoch 90 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.278\n",
            "Validation Loss: 0.563\n",
            "\n",
            " Epoch 91 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.222\n",
            "Validation Loss: 0.731\n",
            "\n",
            " Epoch 92 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.428\n",
            "Validation Loss: 0.892\n",
            "\n",
            " Epoch 93 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.295\n",
            "Validation Loss: 0.511\n",
            "\n",
            " Epoch 94 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.211\n",
            "Validation Loss: 0.760\n",
            "\n",
            " Epoch 95 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.518\n",
            "Validation Loss: 0.647\n",
            "\n",
            " Epoch 96 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.300\n",
            "Validation Loss: 0.535\n",
            "\n",
            " Epoch 97 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.200\n",
            "Validation Loss: 0.765\n",
            "\n",
            " Epoch 98 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.290\n",
            "Validation Loss: 0.563\n",
            "\n",
            " Epoch 99 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.092\n",
            "Validation Loss: 0.519\n",
            "\n",
            " Epoch 100 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.205\n",
            "Validation Loss: 0.469\n",
            "\n",
            " Epoch 101 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.171\n",
            "Validation Loss: 0.579\n",
            "\n",
            " Epoch 102 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.283\n",
            "Validation Loss: 0.443\n",
            "\n",
            " Epoch 103 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.183\n",
            "Validation Loss: 0.467\n",
            "\n",
            " Epoch 104 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.191\n",
            "Validation Loss: 0.427\n",
            "\n",
            " Epoch 105 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.133\n",
            "Validation Loss: 0.526\n",
            "\n",
            " Epoch 106 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.115\n",
            "Validation Loss: 0.444\n",
            "\n",
            " Epoch 107 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.113\n",
            "Validation Loss: 0.512\n",
            "\n",
            " Epoch 108 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.195\n",
            "Validation Loss: 0.479\n",
            "\n",
            " Epoch 109 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.162\n",
            "Validation Loss: 0.532\n",
            "\n",
            " Epoch 110 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.127\n",
            "Validation Loss: 0.464\n",
            "\n",
            " Epoch 111 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.110\n",
            "Validation Loss: 0.474\n",
            "\n",
            " Epoch 112 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.142\n",
            "Validation Loss: 0.714\n",
            "\n",
            " Epoch 113 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.170\n",
            "Validation Loss: 0.658\n",
            "\n",
            " Epoch 114 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.155\n",
            "Validation Loss: 0.500\n",
            "\n",
            " Epoch 115 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.132\n",
            "Validation Loss: 0.518\n",
            "\n",
            " Epoch 116 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.147\n",
            "Validation Loss: 0.553\n",
            "\n",
            " Epoch 117 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.136\n",
            "Validation Loss: 0.529\n",
            "\n",
            " Epoch 118 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.088\n",
            "Validation Loss: 0.673\n",
            "\n",
            " Epoch 119 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.162\n",
            "Validation Loss: 0.508\n",
            "\n",
            " Epoch 120 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.151\n",
            "Validation Loss: 0.515\n",
            "\n",
            " Epoch 121 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.141\n",
            "Validation Loss: 0.614\n",
            "\n",
            " Epoch 122 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.113\n",
            "Validation Loss: 0.458\n",
            "\n",
            " Epoch 123 / 150\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZwkxtFPktRm"
      },
      "source": [
        "Load saved models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5E94MyhjYWX"
      },
      "outputs": [],
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RONM7eGKklKq"
      },
      "source": [
        "Get predictions for text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4TRjDssCsjy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvlk6CTEk3tm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji9SOyPUk35N"
      },
      "outputs": [],
      "source": [
        "\n",
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcZl7uqF9IXk"
      },
      "source": [
        "##SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH8osk9DFCs5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Preprocessing\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "y = df['label']\n",
        "\n",
        "# SVM classifier \n",
        "clf = svm.SVC(probability = True, kernel='rbf')\n",
        "\n",
        "# K-fold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "accuracies = []\n",
        "for train_index, test_index in kfold.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate mean accuracy\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqOz6XCOC9PO"
      },
      "source": [
        "##Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP_Scm2JHD1S"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTjAzOWzG_lq"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "y = df['label']\n",
        "\n",
        "# Random forest classifier \n",
        "\n",
        "rf = RandomForestClassifier(n_jobs = 2)# K-fold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "accuracies = []\n",
        "for train_index, test_index in kfold.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the classifier\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = rf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate mean accuracy\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1u1ZH99Ggv_"
      },
      "source": [
        "##XGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8shWPh-0Ggbx"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbqAwo0XGv8H"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "y = df['label']\n",
        "\n",
        "# Extreme gradient boosting classifier \n",
        "gb = XGBClassifier()\n",
        "# K-fold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "accuracies = []\n",
        "for train_index, test_index in kfold.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the classifier\n",
        "    gb.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = gb.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate mean accuracy\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F3BgQaWuePK"
      },
      "source": [
        "##Deep learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqywsyC2unH0"
      },
      "source": [
        "##LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-UGTEY8upH3"
      },
      "outputs": [],
      "source": [
        "!pip install gensim 3.8.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVlxxrORyh6B"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMOOxDQscPCM"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFJbG6MUcT51"
      },
      "outputs": [],
      "source": [
        "import nltk \n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpSnJfdtvnTL"
      },
      "outputs": [],
      "source": [
        "y = df[\"label\"].values\n",
        "#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process\n",
        "X = []\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in df[\"text\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "    for sent in sentences:\n",
        "        sent = sent.lower()\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        filtered_words = [w.strip() for w in tokens if len(w) > 1]\n",
        "        tmp.extend(filtered_words)\n",
        "    X.append(tmp)\n",
        "\n",
        "del df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYY6F2r8wVXZ"
      },
      "outputs": [],
      "source": [
        "X[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPUj2akHvP5b"
      },
      "source": [
        "Use word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWFFuHE9vfUI"
      },
      "source": [
        "\n",
        "Factorization - Word2Vec\n",
        "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google. Word embedding is the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WOYInZ5diYWZ"
      },
      "outputs": [],
      "source": [
        "#Dimension of vectors we are generating\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
        "w2v_model = gensim.models.Word2Vec(sentences=X, vector_size=EMBEDDING_DIM, window=5, min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HIJaUBI1zGxs",
        "outputId": "e20b1222-7c94-4d2f-c30a-aa86f9471c43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5425"
            ]
          },
          "execution_count": 204,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#vocab size\n",
        "len(w2v_model.wv)\n",
        "\n",
        "#We have now represented each of 122238 words by a 100dim vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kUIqhJAo0Q1f"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import LSTM,Embedding,Bidirectional\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yUBeYERyvUOa"
      },
      "outputs": [],
      "source": [
        "# Tokenizing Text -> Repsesenting each word by a number\n",
        "# Mapping of orginal word to number is preserved in word_index property of tokenizer\n",
        "\n",
        "#Tokenized applies basic processing like changing it to lower case, explicitely setting that as False\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WhilolCX0TMr",
        "outputId": "ae5363d9-d48a-4b9c-fc56-129a21a828f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10, 44, 4, 439, 350, 440, 5, 2739, 351, 26]"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# lets check the first 10 words of first news\n",
        "#every word has been represented with a number\n",
        "X[0][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5wgzoBLj0a8I",
        "outputId": "43b80188-9c2a-44c0-f5d4-29fac30ff70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the -> 1\n",
            "you -> 2\n",
            "and -> 3\n",
            "to -> 4\n",
            "that -> 5\n",
            "of -> 6\n",
            "it -> 7\n",
            "is -> 8\n",
            "in -> 9\n",
            "so -> 10\n"
          ]
        }
      ],
      "source": [
        "#Lets check few word to numerical reprsentation\n",
        "#Mapping is preserved in dictionary -> word_index property of instance\n",
        "word_index = tokenizer.word_index\n",
        "for word, num in word_index.items():\n",
        "    print(f\"{word} -> {num}\")\n",
        "    if num == 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PQBGUG1l0ft7",
        "outputId": "7eb18a40-fbef-471b-f54e-76ab88c4a90c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# For determining size of input...\n",
        "\n",
        "# Making histogram for no of words in news shows that most news article are under 700 words.\n",
        "# Lets keep each news small and truncate all news to 700 while tokenizing\n",
        "plt.hist([len(x) for x in X], bins=500)\n",
        "plt.show()\n",
        "\n",
        "# Its heavily skewed. There are news with 5000 words? Lets truncate these outliers :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZPrJUwEg0n06",
        "outputId": "a993d94c-665e-4080-b4dd-16bd5dc16b11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZpZEAMFNvUXp",
        "outputId": "e2baf033-7d49-43ed-c80c-23974a386aa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nos = np.array([len(x) for x in X])\n",
        "len(nos[nos  < 700])\n",
        "## Out of 44k news, 43k have less than 700 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA4isEB7Lmwi"
      },
      "outputs": [],
      "source": [
        "!pip install Keras-Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdFMfRAVvUgM"
      },
      "outputs": [],
      "source": [
        "from keras_preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4BOcOeo08aX"
      },
      "outputs": [],
      "source": [
        "#Lets keep all text to 700, add padding to news with less than 700 words and truncating long ones\n",
        "maxlen = 700 \n",
        "\n",
        "#Making all news of size maxlen defined above\n",
        "X = pad_sequences(X, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j-KOr3t08rm"
      },
      "outputs": [],
      "source": [
        "#all text has 700 words (in numerical form now). If they had less words, they have been padded with 0\n",
        "# 0 is not associated to any word, as mapping of words started from 1\n",
        "# 0 will also be used later, if unknows word is encountered in test set\n",
        "len(X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1UgeYX009Ee"
      },
      "outputs": [],
      "source": [
        "# Adding 1 because of reserved 0 index\n",
        "# Embedding Layer creates one more vector for \"UNKNOWN\" words, or padded words (0s). This Vector is filled with zeros.\n",
        "# Thus our vocab size inceeases by 1\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOMj04wS09Pq"
      },
      "outputs": [],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg8KPmvd1xiD"
      },
      "outputs": [],
      "source": [
        "# Function to create weight matrix from word2vec gensim model\n",
        "def get_weight_matrix(model, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = model.wv[word]\n",
        "    return weight_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qekR2Ee13xz"
      },
      "source": [
        "We Create a matrix of mapping between word-index and vectors. We use this as weights in embedding layer Embedding layer accepts numecical-token of word and outputs corresponding vercor to inner layer. It sends vector of zeros to next layer for unknown words which would be tokenized to 0. Input length of Embedding Layer is the length of each news (700 now due to padding and truncating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDu4yfzu09Yw"
      },
      "outputs": [],
      "source": [
        "#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\n",
        "embedding_vectors = get_weight_matrix(w2v_model, word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v51Qdlg39qm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential \n",
        "from keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g7--4kz09jz"
      },
      "outputs": [],
      "source": [
        "#Defining Neural Network\n",
        "model = Sequential()\n",
        "#Non-trainable embeddidng layer\n",
        "model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n",
        "#LSTM \n",
        "model.add(LSTM(units=128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "del embedding_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LHOWSqf4Jul"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8MLybBC4SSW"
      },
      "outputs": [],
      "source": [
        "#Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ko28gsi09r7"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, validation_split=0.3, epochs=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEFWOsNa4aDt"
      },
      "outputs": [],
      "source": [
        "#Prediction is in probability of news being real, so converting into classes\n",
        "# Class 0 (Fake) if predicted prob < 0.5, else class 1 (Real)\n",
        "y_pred = (model.predict(X_test) >= 0.5).astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOJ6Jqol5NJn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiTXszGD4kxx"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "047pbGXB6qMm"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt9_amwe6wZf"
      },
      "outputs": [],
      "source": [
        "# save it as a h5 file\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model.save('model_lstm.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pByqyYaD4k_j"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiiqFrr07Kst"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7raoQHZ09zk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu5RFeEM7Oig"
      },
      "source": [
        "##Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydOaf1uD7j1M"
      },
      "outputs": [],
      "source": [
        "#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\n",
        "embedding_vectors = get_weight_matrix(w2v_model, word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAmqj86C7Yx6"
      },
      "outputs": [],
      "source": [
        "#Defining Neural Network\n",
        "model_b = Sequential()\n",
        "#Non-trainable embeddidng layer\n",
        "model_b.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n",
        "#Bidirectional LSTM \n",
        "model_b.add(Bidirectional(LSTM(128)))\n",
        "\n",
        "model_b.add(Dense(1, activation='sigmoid'))\n",
        "model_b.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "del embedding_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae_eTtV_7Y-b"
      },
      "outputs": [],
      "source": [
        "model_b.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1DAaj6d7ZHY"
      },
      "outputs": [],
      "source": [
        "#Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN-6a60G7vZI"
      },
      "outputs": [],
      "source": [
        "model_b.fit(X_train, y_train, validation_split=0.3, epochs=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq0GZjt57viw"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Prediction is in probability of news being real, so converting into classes\n",
        "# Class 0 (Fake) if predicted prob < 0.5, else class 1 (Real)\n",
        "y_pred = (model_b.predict(X_test) >= 0.5).astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw7nFRbk7vqt"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLp627tg7ZO4"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08HlW6lr8fs8"
      },
      "outputs": [],
      "source": [
        "# save it as a h5 file\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_b.save('model_blstm.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA-VZwzP8nOP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcJ0F68q7ZWH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5h5E1DXWfqD"
      },
      "source": [
        "#Table of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlFAOy3-mpcW"
      },
      "source": [
        "For English language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQHiAHFoWo1S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "Result_Table = pd.DataFrame({'Model':['Bert','LSTM','BI-LSTM','RF','XGB'],'Accuracy (%)':[91,61,83, 98,94]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ht5BoeAFWmuF",
        "outputId": "ad72fea6-bc89-4f51-dc35-d792e5a5973f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ae99ed92-fe7a-4d29-b6e1-0f280d4ea1cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bert</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BI-LSTM</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RF</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGB</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae99ed92-fe7a-4d29-b6e1-0f280d4ea1cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae99ed92-fe7a-4d29-b6e1-0f280d4ea1cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae99ed92-fe7a-4d29-b6e1-0f280d4ea1cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Model  Accuracy (%)\n",
              "0     Bert            91\n",
              "1     LSTM            61\n",
              "2  BI-LSTM            83\n",
              "3       RF            98\n",
              "4      XGB            94"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Result_Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfcBw7bpnIua"
      },
      "source": [
        "For kiswahili language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adful3Z2nMbX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "Result_Table = pd.DataFrame({'Model':['Bert','LSTM','BI-LSTM','SVM','RF','XGB'],'Accuracy (%)':[82,70,74,81,83,85]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "5bK-D-cnnNPd",
        "outputId": "9f0d5147-b785-4930-ed0d-fac29e86c7b9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b063de58-370b-431d-8acb-e3c8e7157427\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bert</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BI-LSTM</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SVM</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RF</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGB</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b063de58-370b-431d-8acb-e3c8e7157427')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b063de58-370b-431d-8acb-e3c8e7157427 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b063de58-370b-431d-8acb-e3c8e7157427');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Model  Accuracy (%)\n",
              "0     Bert            82\n",
              "1     LSTM            70\n",
              "2  BI-LSTM            74\n",
              "3      SVM            81\n",
              "4       RF            83\n",
              "5      XGB            85"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Result_Table"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fRhTDO1k_XYG",
        "uRAJhSh8_3Fc",
        "EJR5yDnN__Y-",
        "z2wD6vDZ5v0w",
        "oI35lW4SQrdq",
        "jdf3ncAElahT",
        "DcxrLS1_lH5X",
        "tcZl7uqF9IXk",
        "PqOz6XCOC9PO",
        "Yu5RFeEM7Oig",
        "H5h5E1DXWfqD"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}